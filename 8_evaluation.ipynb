# Importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assume ensemble_majority_preds and ensemble_weighted_preds were loaded from previous steps, alongside true_labels
# Replace with actual loading if necessary

# Example loaded variables
# ensemble_majority_preds = np.load("path/to/ensemble_majority_preds.npy")
# ensemble_weighted_preds = np.load("path/to/ensemble_weighted_preds.npy")
# true_labels = np.load("path/to/true_labels.npy")

# Evaluation Metrics Function
def evaluate_model_metrics(true_labels, predicted_labels, method_name):
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels)
    recall = recall_score(true_labels, predicted_labels)
    f1 = f1_score(true_labels, predicted_labels)
    print(f"\n{method_name} Performance:")
    print(f"Accuracy: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    return accuracy, precision, recall, f1

# Evaluating Majority Voting Ensemble
accuracy_mv, precision_mv, recall_mv, f1_mv = evaluate_model_metrics(true_labels, ensemble_majority_preds, "Majority Voting Ensemble")

# Evaluating Weighted Averaging Ensemble
accuracy_wa, precision_wa, recall_wa, f1_wa = evaluate_model_metrics(true_labels, ensemble_weighted_preds, "Weighted Averaging Ensemble")

# Plotting Confusion Matrix
def plot_confusion_matrix(true_labels, predicted_labels, method_name):
    cm = confusion_matrix(true_labels, predicted_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"{method_name} - Confusion Matrix")
    plt.show()

# Confusion Matrix for Majority Voting Ensemble
plot_confusion_matrix(true_labels, ensemble_majority_preds, "Majority Voting Ensemble")

# Confusion Matrix for Weighted Averaging Ensemble
plot_confusion_matrix(true_labels, ensemble_weighted_preds, "Weighted Averaging Ensemble")

# Plotting ROC Curve
def plot_roc_curve(true_labels, predicted_probs, method_name):
    fpr, tpr, _ = roc_curve(true_labels, predicted_probs)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC curve (area = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"{method_name} - ROC Curve")
    plt.legend(loc="lower right")
    plt.show()

# ROC Curve for Weighted Averaging (assuming averaged probabilities for weighted ensemble)
predicted_probs_wa = (0.33 * ensemble_majority_preds + 0.33 * ensemble_weighted_preds) / 2  # example probabilities
plot_roc_curve(true_labels, predicted_probs_wa, "Weighted Averaging Ensemble")
